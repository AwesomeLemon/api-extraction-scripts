# coding=utf-8
import string
from random import random
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem import PorterStemmer
import json

import Java.java_database
import Java.java_dataset_utils as java_dataset_utils
import CSharp.utils as utils
from Java.java_database import parse_database_to_eng_and_api


def create_dict_on_condition(sentence_list, predicate):
    word_set = set()
    for sentence in sentence_list:
        for word in sentence:
            if predicate(word):
                word_set.add(word)
    return word_set


def clean_up_sentences(list_of_sentence_pairs, api_dict=None, if_shorten=False, if_remove_repetitions=False,
                       if_stem=False, target_bayou=True, target_stdlib_android=True, if_remove_xceptions=True,
                       target_jdk=True, if_remove_autogenerated=True):
    # lemmatizer = WordNetLemmatizer()
    stemmer = PorterStemmer()

    bayou_android_classes = set(json.load(open("../bayou_files/android_class.json")))
    bayou_android_funs = set(json.load(open("../bayou_files/android_functions.json")))
    bayou_stdlib_classes = set(json.load(open("../bayou_files/stdlib_class.json")))
    bayou_stdlib_funs = set(json.load(open("../bayou_files/stdlib_functions.json")))
    bayou_stdlib_funs.add("new")
    bayou_android_funs.add("new")
    jdk_classes = set(json.load(open("jdk_classes.json")))

    def clean_eng(sentence_eng):
        sentence_eng = sentence_eng.translate(str.maketrans('-', ' '))
        sentence_eng = sentence_eng.translate(str.maketrans('', '', string.punctuation))
        sentence_eng = sentence_eng.split(' ')
        sentence_eng = [stemmer.stem(word.lower()) if if_stem else word.lower()
                        for word in sentence_eng if 20 > len(word) > 0]
        return sentence_eng
        # for i, word in enumerate(words):
        #     words[i] = word.lower().translate(None, string.punctuation)

    def clean_api_by_dict(sentence_api):
        return [call for call in sentence_api if call in api_dict]

    def clean_api_bayou(sentence_api):
        len_before = len(sentence_api)

        def call_in_bayou_dicts(call):
            def list_contains_set_element(alist, aset):
                for element in alist:
                    if element in aset:
                        return True
                return False

            identifiers = call.split('.')
            return (list_contains_set_element(identifiers, bayou_stdlib_classes)
                    and list_contains_set_element(identifiers, bayou_stdlib_funs)) or \
                   (list_contains_set_element(identifiers, bayou_android_classes)
                    and list_contains_set_element(identifiers, bayou_android_funs))

        cleaned = [call
                   if call.count('.') == 1
                   else call[call.rfind('.', 0, call.rfind('.')) + 1:]
                   for call in sentence_api
                   if (call.startswith("java") or call.startswith("android") or call.count('.') == 1)
                   and call_in_bayou_dicts(call)]
        len_after = len(cleaned)
        threshold = 0.8
        if len_after / float(len_before) < threshold:
            return []
        return cleaned

    def clean_api_jdk(sentence_api):
        def call_in_jdk_dict(call):
            if call.startswith("com.") or call.startswith("io."):
                return False

            last_dot_ind = call.rfind('.')
            second_last_dot_ind = call.rfind('.', 0, last_dot_ind)
            if second_last_dot_ind == -1:
                second_last_dot_ind = -1  # yep, that's how it should be. Just a reminder that because of +1 in the next line this will be 0.
            short_classname = call[second_last_dot_ind + 1:last_dot_ind]
            is_jdk_short_class = short_classname in jdk_classes
            is_jdk_long_class = False
            third_last_dot_ind = call.rfind('.', 0, second_last_dot_ind)
            if third_last_dot_ind != -1:
                longer_classname = call[third_last_dot_ind + 1:last_dot_ind]
                is_jdk_long_class = longer_classname in jdk_classes

            return is_jdk_short_class or is_jdk_long_class

        return [call for call in sentence_api if call_in_jdk_dict(call)]

    def clean_api_stdlib_android(sentence_api):  # sorta like bayou, but without caring for exact functions
        return [call if call.count('.') == 1
                else call[call.rfind('.', 0, call.rfind('.')) + 1:]
                for call in sentence_api
                if (call.startswith("java") or call.startswith("android") or call.count('.') == 1)]

    def clean_api_shorten(sentence_api):
        sentence_word_boundary = 10
        if len(sentence_api) >= sentence_word_boundary:
            words_at_ends_to_keep = 3
            start = sentence_api[:words_at_ends_to_keep]
            end = sentence_api[len(sentence_api) - words_at_ends_to_keep:]
            sentence_api[:] = start + end
        return sentence_api

    def clean_api_exceptions(sentence_api):
        return [call for call in sentence_api if "xception" not in call]

    cleaned_sentences = []
    cnt = 0
    for sentence_pair in list_of_sentence_pairs:
        eng = sentence_pair[0]
        if if_remove_autogenerated:
            if eng.startswith("This method initializes") \
                    or eng.startswith("Called when the activity is first created") or eng.startswith("DOCUMENT ME!") \
                    or eng.startswith("This method is called from within the constructor to initialize the form") \
                    or eng.startswith("initialize the form"):
                continue
        cleaned_eng = clean_eng(eng)
        api = java_dataset_utils.remove_stuff_in_brackets(sentence_pair[1], '<', '>')#no generics
        api = api.split(' ')
        if api_dict is not None:
            api = clean_api_by_dict(api)
        if target_jdk:
            api = clean_api_jdk(api)
        if target_bayou:
            api = clean_api_bayou(api)
        if target_stdlib_android:
            api = clean_api_stdlib_android(api)
        if if_shorten:
            api = clean_api_shorten(api)
        if if_remove_repetitions:
            api = remove_consecutive_repetitions(api)
        if if_remove_xceptions:
            api = clean_api_exceptions(api)
        if len(api) > 0 and len(cleaned_eng) > 0:
            cleaned_sentences.append((cleaned_eng, api))
        cnt += 1
        if cnt % 100000 == 0:
            print(cnt)

    return cleaned_sentences


def remove_consecutive_repetitions(sentence):
    prev = ""
    res = []
    for word in sentence:
        if word != prev:
            res.append(word)
        prev = word
    return res


def separate_to_train_and_dev(eng_api_list):
    total = len(eng_api_list)
    test_size = 20000.0
    prob_boundary = test_size / float(total)
    dev = []
    train = []
    for pair in eng_api_list:
        if random() < prob_boundary:
            dev.append(pair)
        else:
            train.append(pair)
    return train, dev


def improve_eng_api(eng_api, ifcleanup=True, leave_only_system=False, make_unique=True, if_stem=False):
    if ifcleanup:
        api_dict = None
        if leave_only_system:
            _, api_desc = zip(*eng_api)
            api_dict = create_dict_on_condition(api_desc, lambda x: x.startswith("System.") or x.count('.') == 1)
        eng_api = clean_up_sentences(eng_api, api_dict, if_remove_repetitions=False, if_stem=if_stem,
                                     target_bayou=False,
                                     target_stdlib_android=False, if_remove_xceptions=False, target_jdk=True,
                                     if_remove_autogenerated=False)
    if make_unique:
        print('before unique: ' + str(len(eng_api)))
        return list_to_unique(eng_api)
    return eng_api


def list_to_unique(eng_api):
    un1 = list(set(map((lambda x: (tuple(x[0]), tuple(x[1]))), eng_api)))
    # eng_api = list(map((lambda x: (' '.join(x[0]), ' '.join(x[1]))), eng_api))
    # un2 = list(set(eng_api))
    return un1


def write_all_train_data_to_files(english_desc, api_desc, engfile="eng.txt", apifile="api.txt", ifoverwrite=False):
    def write_train_data_to_file(lines, filepath, file_open_modifier):
        with open(filepath, file_open_modifier, encoding='utf-8') as f:
            for sentence in lines:
                f.write(" ".join(sentence) + "\n")

    file_open_modifier = 'w' if ifoverwrite else 'a'
    write_train_data_to_file(english_desc, engfile, file_open_modifier)
    write_train_data_to_file(api_desc, apifile, file_open_modifier)


def parse_database_comments_to_train_and_test(filter_langs=True, many_stars_only=False):
    if many_stars_only:
        eng_api = Java.java_database.parse_database_to_eng_and_api_reps_many_stars()
    else:
        eng_api = parse_database_to_eng_and_api(filter_langs=filter_langs)
    eng_api = improve_eng_api(eng_api, if_stem=False)

    return to_separate_eng_api_train_dev(eng_api)


def parse_loaded_database_comments_to_train_and_test():
    import pickle
    with open('/home/jet/PycharmProjects/process_github_data/Java/eng_api.pkl', 'rb') as f:
        eng_api = pickle.load(f)
    eng_api = improve_eng_api(eng_api, if_stem=False)

    return to_separate_eng_api_train_dev(eng_api)


def save_parse_database_to_eng_and_api():
    eng_api = parse_database_to_eng_and_api(filter_langs=False)
    import yaml
    file_cnt = 100
    import math
    file_lens = math.ceil(len(eng_api) / float(file_cnt))
    for i in range(file_cnt - 1):
        with open('/media/jet/HDD/eng_apis/eng_api_orig' + str(i) + '.yml', 'w') as f:
            yaml.dump(eng_api[i * file_lens:(i + 1) * file_lens], f)

    with open('/media/jet/HDD/eng_apis/eng_api_orig' + str(file_cnt - 1) + '.yml', 'w') as f:
        yaml.dump(eng_api[(file_cnt - 1) * file_lens:])


def parse_loaded_from_yml_to_train_and_test():
    import yaml
    file_cnt = 99
    eng_api = []
    for i in range(file_cnt):
        print(i)
        with open('/media/jet/HDD/eng_apis/eng_api_orig' + str(i) + '.yml', 'r') as f:
            eng_api.extend(yaml.load(f))
    print("Total before: " + str(len(eng_api)))
    eng_api = improve_eng_api(eng_api, if_stem=False)
    return to_separate_eng_api_train_dev(eng_api)


def to_separate_eng_api_train_dev(eng_api):
    train, dev = separate_to_train_and_dev(eng_api)
    eng_train, api_train = zip(*train)
    eng_dev, api_dev = zip(*dev)
    return (eng_train, eng_dev), (api_train, api_dev)


def parse_database_to_cleaned_eng_api():
    eng_api = parse_database_to_eng_and_api()
    eng_api = improve_eng_api(eng_api)

    return eng_api


def create_vocab(sentences, vocab_size, skip_first_n_words):
    vocab = {}
    for sentence in sentences:
        for w in sentence:
            if w in vocab:
                vocab[w] += 1
            else:
                vocab[w] = 1
    vocab_list = sorted(vocab, key=vocab.get, reverse=True)
    if len(vocab_list) > vocab_size:
        vocab_list = vocab_list[skip_first_n_words:vocab_size]
    return vocab_list


def filter_list_of_lists_by_vocab(list_of_lists, vocab):
    vocab = set(vocab)
    temp = [list(sentence) for sentence in list_of_lists]
    for sentence in temp:
        sentence[:] = [word for word in sentence if word in vocab]
    return [sentence for sentence in temp if len(sentence) > 0]


def filter_eng_api_by_vocabs_and_remove_consecutive_api_repetitions(eng, api, vocab_eng, vocab_api,
                                                                    if_remove_repetitions=False):
    vocab_eng = set(vocab_eng)
    vocab_api = set(vocab_api)
    res_e = []
    res_a = []
    for (eng_sent, api_sent) in zip(eng, api):
        e = [word for word in eng_sent if word in vocab_eng]
        a = [word for word in api_sent if word in vocab_api]
        if if_remove_repetitions:
            a = remove_consecutive_repetitions(a)
        if len(e) > 1 and len(a) > 0:
            res_e.append(e)
            res_a.append(a)
    return res_e, res_a


def write_two_vocabs(eng_vocab, api_vocab, eng_size, api_size, vocab_postfix=''):
    utils.write_lines_to_file('vocab' + str(eng_size) + '_' + vocab_postfix + '.from', eng_vocab)
    utils.write_lines_to_file('vocab' + str(api_size) + '_' + vocab_postfix + '.to', api_vocab)


def main():
    dataset_index = 'java_orig_jdk'  # java_bayou10k10k_allimproves
    train_eng_file = 'train' + dataset_index + '.eng'
    train_api_file = 'train' + dataset_index + '.api'
    test_eng_file = 'test' + dataset_index + '.eng'
    test_api_file = 'test' + dataset_index + '.api'
    # (eng_train_samples, eng_test_samples), (api_train_samples, api_test_samples) = save_parse_database_to_eng_and_api()
    # (eng_train_samples, eng_test_samples), (api_train_samples, api_test_samples) = parse_database_comments_to_train_and_test()
    (eng_train_samples, eng_test_samples), (
    api_train_samples, api_test_samples) = parse_loaded_from_yml_to_train_and_test()
    # (eng_train_samples, eng_test_samples), (api_train_samples, api_test_samples) = parse_database_comments_to_train_and_test_reps_many_stars()
    eng_vocab_size = 10000
    eng_vocab = create_vocab(eng_train_samples, eng_vocab_size, 0)
    api_vocab_size = 10000
    api_vocab = create_vocab(api_train_samples, api_vocab_size, 0)
    write_two_vocabs(eng_vocab, api_vocab, eng_vocab_size, api_vocab_size,
                     vocab_postfix=dataset_index)

    print('before vocab frequency filtering: ' + str(len(eng_train_samples)))
    eng_train_samples, api_train_samples = filter_eng_api_by_vocabs_and_remove_consecutive_api_repetitions(
        eng_train_samples, api_train_samples, eng_vocab, api_vocab, if_remove_repetitions=False)
    eng_test_samples, api_test_samples = filter_eng_api_by_vocabs_and_remove_consecutive_api_repetitions(
        eng_test_samples, api_test_samples, eng_vocab, api_vocab, if_remove_repetitions=False)

    print('after vocab frequency filtering: ' + str(len(eng_train_samples)))

    write_all_train_data_to_files(eng_train_samples, api_train_samples, train_eng_file, train_api_file,
                                  ifoverwrite=True)
    write_all_train_data_to_files(eng_test_samples, api_test_samples, test_eng_file, test_api_file, ifoverwrite=True)


if __name__ == "__main__":
    main()
    # save_parse_database_to_eng_and_api()
    # load_eng_api()
    # test()